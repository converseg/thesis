\chapter{Estimating IRT Parameters with Variational Autoencoders}
The primary contribution of this thesis is the development of a method for IRT parameter estimation which uses a modified VAE. The method, titled ``ML2P-VAE'', is interesting from multiple perspectives. In the application area, it is an unconventional approach can produce estimates as accurate as those of traditional parameter estimation techniques. Further, ML2P-VAE scales much better than traditional methods as the number of latent abilities becomes large. In the field of machine learning, ML2P-VAE is an unsupervised learning technique which yields explainable results. A variational autoencoder is used in an unorthodox way, and the trainable parameters and a hidden neural layer are able to be understood in a real-world context.

\section{ML2P-VAE Method Description}\label{sec:ml2p_vae}
Assume we are given an assessment with $n$ items which tests $K$ latent skills, and that $N$ students take this exam. Data is given as an $N \times n$ binary matrix, as in Equation \ref{eq:responses}. No information about student latent ability parameters $\Theta \in \R^K$ or item parameters $a_{ik}$ and $b_i$ is provided. However, there is access to an expert-annotated binary $Q$-matrix detailing the item-skill associations, as in Equation \ref{eq:q_matrix}.

\sideremark{TODO: relate to IRT background section and VAE derivation}

A number of specifications are required in order to use a VAE as an IRT parameter estimation method. First, set up the neural network so that the input and output layers each have $n$ nodes, each representing one item. The inputs are be the binary response vectors $\vect u_j$ and the outputs are approximations of the probability of student $j$ answering each item correctly. Next, the dimension of the hidden distribution (the output of the encoder) must be equal to $K$, the number of latent skills. The usual VAE loss function described in Equation \ref{eq:vae_loss} is still used to optimize ML2P-VAE.
The modifications to a typical VAE architecture are focused on the decoder. No hidden layers are used in the decoder. Instead, a non-dense set of weights connects the decoder input to the decoder output. The non-zero weights here are determined by the $Q$-matrix; recall that the input to the decoder has $K$ nodes, the decoder output has $n$ nodes, and $Q \in \{0,1\}^{n \times K}$. So for a trainable weight $w_{ik}$ in the VAE decoder, if $q_{ik}=0$, then fix $w_{ik}=0$ as well, and it will never be updated during the training process. This modification is key to allowing interpretation of the hidden latent distribution of a VAE.

The ML2P-VAE method requires the use of the sigmoid activation function
\begin{equation}
  \sigma(z_i) = \frac{1}{1 + e^{z_i}}
  \label{eq:sigmoid}
\end{equation}
in the output layer. Here, $z_i = \sum_{k=1^K} w_{ik}\alpha_{k} + \beta_i$, where $w_{ik}$ is the weight between the $k$-th and $i$-th nodes in the decoder input and decoder output layer, $\alpha_k$ is the activation of the $k$-th node in the decoder input layer, and $\beta_i$ is the additive bias in the output layer. Note the similarity between Equation \ref{eq:sigmoid} and Equation \ref{eq:ml2p}. The constraint on the weights along with the sigmoid activation function allows for interpretation of the decoder as an ML2P model.

Specifically, the decoder weights $w_{ik}$ can be interpreted as estimates to the discrimination parameters $a_{ik}$, the output bias $\beta_i$ can be interpreted as estimates to the difficulty parameters $b_i$, and the activations $\alpha_k$ produced by the encoder (given response input $\vect u_j$) can be interpreted as estimates to the student ability parameter $\theta_{kj}$. \sideremark{may want to mention that the value outputted by encoder is then sampled from for decoder input}

Further modifications can improve the performance of ML2P-VAE. In IRT, discrimination parameters are assumed to be non-negative, because an increase in a skill should never decrease the probability of answering an item correctly. With this assumption in mind, requiring all decoder weights $w_{ik} \geq 0$ avoids a potential identification problem. \sideremark{explain what this means $-\theta a_{ik} = \theta (-a_{ik})$}

Relating the ML2P-VAE model back to the derivation of a variational autoencoder in Section \ref{sec:vae_derive}, the notations used are related as follows. The response vectors $\vect u_j$ serve as the observed data $\vect x$, and the latent ability of students $\vect \Theta_j$ serve as the latent code $\vect z$. The modified decoder, which produced outputs the probabilities of student $j$ answering item $i$ correctly $\hat P_{ij} = \sigma(\theta_{ij})$, as described in Equation \ref{eq:sigmoid}, serves as the observed input reconstruction $\hat x_i$ in Equation \ref{eq:bernoulli}.

\subsection{Full Covariance Matrix Implementation}\label{sec:cov}
There are many publicly available code examples of VAE implementations which assume the latent space follows a standard normal distribution $\mathcal{N}(0,I)$. But it is not so common to train a VAE which assumes the latent prior $p(\Theta)$ has correlated dimensions. Since most applications do not attempt to interpret hidden layers of a VAE, there is no available information on the correlations of abstract, unobservable features. Additionally, it is often beneficial to force the latent dimensions to be independent of one another.

In IRT, we may be able to quantify the correlation between latent abilities, presenting need for the ML2P-VAE model to take advantage of this information. This task is nontrivial due to two mechanisms of the VAE:
\begin{itemize}
  \item[(1)] sampling from the learned distribution, and
  \item[(2)] calculating Kullback-Leibler Divergence
\end{itemize}
These two characteristics must be addressed when constructing the neural architecture.

After training a VAE, sending a data point $u_0$ through the encoder needs to give a set of values that correspond to a probability distribution. For a $K$-dimensional multivariate Gaussian distribution, these values are a vector $\mu_0 \in \R^K$ and a symmetric, positive-definite matrix $\Sigma_0 \in \R^{K\times K}$. Sampling from $\mathcal{N}(\mu_0, \Sigma_0)$ requires a matrix $G_0$ such that $G_0 G_0^\top = \Sigma_0$. This matrix factorization $G_0$ is not unique, but it can be convenient to use the Cholesky decomposition of $\Sigma_0$ \cite{atkinson}. The sample of the multivariate Gaussian is calculated as $z_0 = \mu_0 + G_0 \vect{\e_0}$, where $\vect{\e_0} = (\e_1, \ldots, \e_K)^\top$ and $\e_i \sim \mathcal{N}(0,1)$. 

The KL-Divergence between two $K$-variate Gaussian distributions is given as
\begin{equation}
\begin{split}
  &\mathcal{D}_{KL}\left[\mathcal{N}(\mu_0, \Sigma_0) || \mathcal{N}(\mu_1, \Sigma_1) \right] = \\
&\frac{1}{2} \left( \tr(\Sigma_1^{-1}) \Sigma_0) + (\mu_1 - \mu_0) \Sigma_1^{-1} (\mu_1 - \mu_0) - K + \ln\left( \frac{\det \Sigma_1}\det \Sigma_0 \right) \right)
  \label{eq:kl_multivariate}
\end{split}
\end{equation}
When using this in a VAE, $\mathcal{N}(\mu_1, \Sigma_1)$ corresponds to the prior $p(\Theta)$, and so $\mu_1$ and $\Sigma_1$ are constant. Then $\Sigma_1^{-1}$ only needs to be computed once, and this matrix inversion won't cause computation time problems at any point. Note that Equation \ref{eq:kl_multivariate} computes $\ln \det \Sigma_0$, so we must have $\det \Sigma_0 > 0$ at any point during training. Recall that $\mu_0$ and $\Sigma_0$ correspond to the input $u_0$, and also depend on all the trainable weights and biases in the VAE encoder. These parameters are usually initialized randomly, and the user has little control over their values during training. If $\det \Sigma_0 \leq 0$ for any input $u_0$ at any point during training, then it is not possible to compute the loss and gradient. Thus, a specific architecture which guarantees that $\det \Sigma_0 > 0$, regardless of the input $u_0$ or encoder parameters, is required.

This architecture is described as follows. The input and output to the neural network consists of $n$ nodes, each representing an item on an assessment. After a sufficient number of hidden layers of sufficient size, the encoder outputs $K + \frac{K(K+1)}{2}$ nodes. The first $K$ nodes represent the mean vector $\mu_0$, and the remaining $\frac{K(K+1)}{2}$ nodes are arranged into a lower triangular matrix $L_0 \in \R^{K\times K}$. The covariance matrix is obtained by using the matrix exponential $\Sigma_0 = e^{L_0} \cdot \left( e^{L_0} \right)^\top$.

\begin{theorem}
  $\Sigma_0$ constructed as described previously is symmetric, positive-definite, and has positive determinant.
  \label{thm:cov_vae}
\end{theorem}
\begin{proof}
  Consider any lower triangular $L_0 \in \R^{K\times K}$. Define 
  \[G_0 = e^{L_0} = \sum_{n=1}^\infty \frac{L_0^n}{n!} = I + L_0 + \frac{1}{2} L_0 \cdot L_0 + \cdots\]
  $G_0$ is lower triangular, since addition and multiplication of matrices preserve this property. Further, $G_0$ is nonsingular, because $\det G_0 = \det(e^{L_0}) = e^{\tr L_0} > 0$.

  Set $\Sigma_0 = G_0 G_0^\top$. Clearly, $\Sigma_0$ is symmetric as $\Sigma_0^\top = (G_0 G_0^\top)^\top = G_0 G_0^\top = \Sigma_0$. Further, $\det \Sigma_0 = \det G_0 \cdot \det G_0^\top > 0$. So now for any nonzero $x \in \R^K$,
  \[\langle \Sigma_0 x, x \rangle = x^\top \Sigma_0 x = x^\top G_0 G_0^\top x = \langle G_0^\top x, G_0^\top x \rangle = ||G_0 x||_2^2 > 0\]
  Therefore, $\Sigma_0$ is positive-definite.
\end{proof}

Theorem \ref{thm:cov_vae} shows that in this specific neural network architecture, $\Sigma_0$ can be interpreted as a covariance matrix. Thus, the VAE encoder maps a data point $u_0$ to a multivariate Gaussian distribution $\mathcal{N}(\mu_0, \Sigma_0)$. Additionally, the sampling operation and KL-Divergence calculation can always be carried out without issue. A visualization of the ML2P-VAE architecture for correlated latent traits is shown in Figure \ref{fig:ml2pvae_visual}.

\begin{figure}[h]
  \centering
  \includegraphics[width=.85\textwidth]{img/ml2pvae_visual.png}
  \caption{Visualization of the ML2P-VAE architecture for two correlated latent traits and six input items. Note that the trainable weights matrix in the decoder is not dense, but is determined by the given $Q$-matrix.}
  \label{fig:ml2pvae_visual}
\end{figure}


\subsection{Variants of ML2P-VAE}\label{sec:variants}

We consider three scenarios for using ML2P-VAE in practice: (a) the best case scenario where we assume that the covariance matrix between all latent traits is known, (b) we don't know the exact covariance matrix, so it is estimated using other methods, and (c) we simply assume that all traits are independent. These three situations result in three variations of the ML2P-VAE method: ML2P-VAE$_{full}$, ML2P-VAE$_{est}$, and ML2P-VAE$_{ind}$, respectively. 

In scenario (b), we multiply the response matrix ($N$ students by $n$ items) by the $Q$-matrix ($n$ items by $K$ abilities). We then take the Pearson correlation of the columns of this $N\times K$ matrix to obtain an approximate correlation matrix in $\R^{K\times K}$ between abilities.

In order to estimate the correlations between latent traits for use of ML2P-VAE$_{est}$, the student response matrix $U \in \R^{N\times n}$ is multiplied by the $Q$-matrix $Q \in \R^{n \times K}$. Then the Pearson correlation of the columns of the resulting matrix produce an approximate correlation matrix $\hat \Sigma \in \R^{K \times K}$: 
\[M = U \cdot Q , \quad M \in \R^{N\times K}\]
\begin{equation}
  \hat \Sigma_{kl} = \frac{\sum_{i=1}^N (k_i - \bar k)(l_i - \bar l)}{\sqrt{\sum_{i=1}^N(k_i - \bar k)^2} \sqrt{\sum_{i=1}^N (l_i - \bar l)^2}}
  \label{eq:approx_cor_mat}
\end{equation}
where $\bar k$ and $\bar l$ are the mean values of the $k$-th and $l$-th columns of $M$, respectively.

A final variation of ML2P-VAE comes when the IRT model to be estimated is changed. If assuming that student responses are generated according to the Rasch model as in Equation \ref{eq:rasch}, rather than the ML2P model as in Equation \ref{eq:ml2p}, then another variation of VAE parameter estimation methods can be considered. A more appropriate name for this alternative estimation method is Rasch-VAE.

Since there are no discrimination parameters in the Rasch model, only item difficulties and student abilities need to be estimated. To account for this, the weights in the VAE decoder are restricted to be \textit{equal} to the $Q$-matrix. This still allows for interpretation of the learned distribution of the VAE as estimates to student abilities $\Theta$, while requiring ``discrimination parameters'' to be equal to either one or zero, fitting more closely to Equation \ref{eq:rasch}.

%\subsection{On Convergence}
%
%\subsubsection{Proving local minimum at true solution}
%\sideremark{Can't really guarantee convergence globally with SGD because neural nets are super nonconvex}
%
%In this section, we investigate how the modified ML2P-VAE model fits into the theoretical derivation of a VAE, as described in Section \ref{sec:vae_derive}.
%
%We define the true and predicted probability of student $j$ answering item $i$ correctly with $P_{ij}$ and $\hat P_{ij}$, respectively. The former comes from the ML2P model, and the latter is the output of a neural network as in Equation \ref{eq:sigmoid}. We assume the more simple case, where $\Theta \sim \mathcal{N}(0,I)$, rather than $\mathcal{N}(\mu, \Sigma)$.
%\begin{equation}
%  P_{ij} = \frac{1}{1 + \exp\left(-\sum_{k=1}^K a_{ik}\theta_{jk} + b_i\right)}
%  \label{eq:ml2p_est}
%\end{equation}
%
%\begin{equation}
%  \hat P_{ij} = \frac{1}{1 + \exp\left(-\sum_{k=1}^K \hat a_{ik} (\hat \theta_{jk} + \e_k \hat \sigma_k) + \hat b_i\right)} 
%    \label{eq:vae_out}
%\end{equation}
%
%Note that $P_{ij}$ is unknown, and we instead have a response sequence $\vect u_j = (u_{1j},\ldots, u_{nj})^\top$ with $u_{ij} = \text{Bern}(P_{ij})$. This also means that $\mathbb{E}[u_{ij}] = P_{ij}$. The variables $\hat a_{ik}$, $\hat b_i$, $\hat \theta_{ik}$, and are parameter estimates from the VAE. The first two are parameters in the neural network, and the ability estimates are taken from feeding responses to the encoder, i.e., $\hat \Theta_j = \text{Encoder}(\vect u_j)$. The noise $\e = (\e_k)_{1\leq k \leq K} \sim \mathcal{N}(0,I)$ is introduced by the sampling operation in the VAE.
%
%\sideremark{Instead of VAE loss (which is really ELBO), this should be the marginal prob. of data (see Kingma/Welling)}
%
%The loss function for a VAE is given by 
%\begin{equation}
%  \begin{split}
%  \mathcal{L}(\vect u_j) &= -\sum_{i=1}^n \left[u_{ij} \log(\hat P_{ij}) + (1-u_{ij})\log(1 - \hat P_{ij})\right] + \mathbb{E}_{q_\alpha(\hat \theta | \vect u_j)}\log\left( \frac{q_{\alpha}(\hat \theta |\vect u_j)}{p(\theta)}\right) \\
%    &= \mathcal{L}_{\text{REC}} + \mathcal{L}_{\text{KL}}
%  \end{split}
%  \label{eq:vae_loss}
%\end{equation}
%We break up the VAE loss into two terms, the reconstruction loss $\mathcal{L}_{\text{REC}}$ and the KL-divergence loss $\mathcal{L}_{\text{KL}}$. in the latter, the distribution $q_\alpha(\hat \theta | \vect u_j)$ is the output of the encoder, and $p(\theta)$ is the assumed prior distribution of $\Theta$, which we set to be $\mathcal{N}(0,I)$.
%
%We write $P_{ij} = \mathbb{E}(u_{ij})$, and similarly define a new ``expected'' loss function:
%\begin{equation}
%  \begin{split}
%  \mathcal{L}_\mathbb{E}(P_j) &= \mathbb{E}_{u_j}[\mathcal{L}(u_{:j})] \\
%  &= -\sum_{i=1}^n [P_{ij} \log(\hat P_{ij}) + (1-P_{ij})\log(1 - \hat P_{ij})] + \mathbb{E}_{q_\alpha(\hat \theta | u_j)}\log\left( \frac{q_{\alpha}(\hat \theta |u_j)}{p(\theta)}\right) \\
%    &= \mathcal{L}_{\mathbb{E}[\text{REC}]} + \mathcal{L}_{\mathbb{E}[\text{KL}]}
%  \end{split}
%  \label{eq:expected_loss}
%\end{equation}
%Notice that calculation of this ``expected loss'' requires the unknown $P_{:j}$. But when we have large amounts of data, we can think of $P_{ij}$ as the average value of the response $u_{ij}$, so using this unknown value here is justified.
%
%Define $z_i = a_{i:} \cdot \theta_{j:} - b_i$ and $\hat z_i = \hat a_{i:} \cdot (\hat \theta_{j:} + \e \cdot \hat \sigma) - \hat b_i$. Note that $z_i$ is fixed, dependent on the data, and does not depend on any parameters of the neural network. $\hat z_:$ is the input to the final layer of the decoder, and the VAE output is $\hat P_{ij} = \sigma(\hat z_i)$, where $\sigma(\cdot)$ is the sigmoid activation function. We compute derivatives of the expected loss function, looking individually at the reconstruction and KL terms. 
%
%\begin{equation}
%  \begin{split}
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i} &= \frac{-1}{1 + e^{-z_i}}\cdot \frac{1}{1 + e^{\hat z_i}} - \frac{1}{1 + e^{z_i}}\cdot \frac{-1}{1 + e^{-\hat z_i}} \\
%  &= \frac{-1}{(1 + e^{-z_i})(1 + e^{\hat z_i})} + \frac{1}{(1 + e^{z_i})(1 + e^{-\hat z_i})} \\
%  &= \frac{-1}{(1 + e^{-a_{i:} \cdot \theta_{j:} + b_i})(1 + e^{\hat a_{i:} \cdot (\hat \theta_{j:} + \e_: \hat \sigma_:) - \hat b_i})} \\
%  &\quad + \frac{1}{(1 + e^{a_{i:} \cdot \theta_{j:} - b_i})(1 + e^{-\hat a_{i:} \cdot (\hat \theta_{j:} + \e_: \hat \sigma_:) + \hat b_i})} 
%\end{split}
%  \label{eq:z_deriv}
%\end{equation}
%
%
%\begin{equation}
%  \begin{split}
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat a_{ik}} &= 
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i}\frac{\partial \hat z_i}{\partial \hat a_{ik}} = \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i} (\hat \theta_{jk} + \e_k \hat \sigma_k) \\
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat b_i} &= 
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i}\frac{\partial \hat z_i}{\partial \hat b_i} = \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i} (-1) \\
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat \theta_{ik}} &= 
%  \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i}\frac{\partial \hat z_i}{\partial \hat \theta_{ik}} = \frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i} (\hat a_{ik}) 
%\end{split}
% \label{eq:param_deriv}
%\end{equation}
%
%Rather than setting these to zero and solving, we show that the most intuitive solution, $\hat a_{ik} = a_{ik}$, $\hat b_i = b_i$, and $\hat \theta_{jk} = \theta_{jk}$, is in fact a minimum of the expected loss function. But first, we must take another expectation over the random variable $\e \sim \mathcal{N}(0,I)$. Obviously, we have that $\mathbb{E}[\e_k] = 0$; this makes our calculations very simple. Notice that we have
%\begin{equation}
%  \begin{split}
%  \mathbb{E}_\e &\left[\frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat z_i}\right] \Big|_{\hat a_{ik} = a_{ik}, \hat b_i = b_i, \hat \theta_{jk} = \theta_{jk}} \\
%  &= \frac{-1}{(1 + e^{-a_{ik}\theta_{jk} + b_i})(1 + e^{a_{ik} (\theta_{kj} + 0 \cdot \hat \sigma_k) - b_i})} + \frac{1}{(1 + e^{a_{ik}\theta_{jk} - b_i})(1 + e^{-a_{ik}(\theta_{jk} + 0 \cdot \hat \sigma_k) + b_i})} \\
%  &= 0
%\end{split}
%\end{equation}
%
%Therefore we clearly have 
%\begin{equation}
%  \begin{split}
%  &\mathbb{E}_\e \left[\frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat a_{ik}}\right] \Big|_{\hat a_{ik} = a_{ik}, \hat b_i = b_i, \hat \theta_{jk} = \theta_{jk}} \\
%  = &\mathbb{E}_\e \left[\frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat b_i}\right] \Big|_{\hat a_{ik} = a_{ik}, \hat b_i = b_i, \hat \theta_{jk} = \theta_{jk}} \\
%  = &\mathbb{E}_\e \left[\frac{\partial \mathcal{L}_{\mathbb{E}[\text{REC}]}}{\partial \hat \theta_{jk}}\right] \Big|_{\hat a_{ik} = a_{ik}, \hat b_i = b_i, \hat \theta_{jk} = \theta_{jk}} \\
%  = &0 \quad \forall i,j,k
%\end{split}
%  \label{solve}
%\end{equation}
%This proves that the true parameters give a local minimum for the expected reconstruction error in the VAE loss.% And because the expected cross-entropy loss function $\mathcal{L}_{\mathbb{E}[\text{REC}]}$ is non-negative, the reconstruction error at the true IRT paramters is a global minimum. \sideremark{any chance this is unique?}
%
%We now consider the Kullback-Leibler divergence term in the expected loss function. Again assuming independent latent traits, we have
%\begin{equation}
%  \mathcal{L}_{KL} = \mathbb{E}_{q(\theta | u)} \log \left( \frac{q(\hat \theta | u)}{p(\theta)} \right) = KL(q(\hat \theta |u) || p(\theta)) = -\frac{1}{2} \sum_{k=1}^K (1 + \log(\hat \sigma_k^2) - \hat \theta_k^2 - \hat \sigma_k^2)
%  \label{eq:kl}
%\end{equation}
%
%It is clear that this regularization term is minimized (and equal to zero) when $\hat \theta_{jk} = 0$ and $\hat \sigma_{jk} = 1$. But what happens when we plug in the ``true'' student ability values as before? We have
%\begin{equation}
%  \mathcal{L}_{KL} \Big|_{\hat \theta = \theta, \hat \sigma = \sigma} = KL(p(\theta | u) || p(\theta))
%  \label{eq}
%\end{equation}
%Notice that this is the KL divergence between the \textbf{true posterior} $p(\theta |u)$ and the \textbf{true prior} $p(\theta)$. This is interpreted as the average difference of number of bits required to encode samples of $p(\theta |u)$ using a code optimized for $p(\theta)$, rather than one optimized for $p(\theta | u)$. \sideremark{We should be okay with accepting this loss, since the true posterior is not actually known, and we are just using the prior as a reference.}
%
%
%
%\subsubsection{Requirements on sparsity of Q-matrix}
%\sideremark{need to look into this idea at some point}
%
%TODO: try to show that this is a global minimum for the full VAE loss function. Also take derivatives of the KL loss w.r.t $\hat \theta_{jk}$. The Q-matrix may help with an identifiability issue (existence of other local minimums) in solving the system $(a_{ik}\theta_{jk} + b_i)_{jk} = z_i$. The Q-matrix \textit{may} make the solution unique.
%
%
%
\section{\textbf{ML2Pvae} Software Package for R}
The ML2P-VAE method for parameter estimation has been compiled in an easy-to-use software package for R \cite{r_package}. This allows researchers who may not have experience with neural networks to implement ML2P-VAE methods on a data set of their choosing. The package \textbf{ML2Pvae} is available on the Comprehensive R Archive Network (CRAN) and can be easily installed using the R command 
\begin{center}
\verb!install.packages(``ML2Pvae'')! 
\end{center}
\sideremark{maybe use lstinline from listings for code}

\subsection{Package Functionality}
\textbf{ML2Pvae} uses Tensorflow and Keras to build and train neural networks, but no knowledge of these libraries are required in order to use \textbf{ML2Pvae}. The package exports 5 functions available to the user. Two of these are used to construct Keras models, with optional parameters specifying the architecture of the neural network. The only parameters which require input from the user are the number of items on the exam, the number of latent abilities that the exam assesses, and the $Q$-matrix relating items and abilities.

The optional inputs in the model construction include a covariance matrix for latent traits, allowing for correlated skills and the implementation described in Section \ref{sec:cov}. An important feature for model selection gives the choice of the number of item parameters to use in the logistic IRT model. Though the package is called \textbf{ML2Pvae} for the Multidimensional Logistic 2-Parameter model, the package allows for estimating parameters with the 1-Parameter Logistic model, also called the Rasch model. In this case, there is only a difficulty parameter for each item; each  discrimination parameter is fixed to be equal to 1. Other options when building ML2P-VAE models specify the number, size and activation functions of the hidden layers in the encoder.

Using the Keras models returned by the construction functions, \textbf{ML2Pvae} provides a function that can be used to train the VAE on data. This function acts as a wrapper for the \verb!fit()! method in the Keras package. The final two methods obtain item parameter estimates and student ability parameter estimates. This is done by grabbing the correct weights/biases from the decoder and feeding student responses through the encoder, respectively. 

\sideremark{Should i include a short code demo?}
